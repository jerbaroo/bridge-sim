{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "# from collections import defaultdict\n",
    "# from copy import deepcopy\n",
    "# from datetime import datetime\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import interp1d\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from fem.run.opensees import OSRunner\n",
    "from classify.data.responses import responses_to_traffic_array\n",
    "from classify.data.traffic import load_traffic\n",
    "from classify.scenario.bridge import healthy_damage, pier_disp_damage\n",
    "from classify.scenario.traffic import normal_traffic\n",
    "# from plot import legend_marker_size\n",
    "from model.bridge import Point\n",
    "from model.bridge.bridge_705 import bridge_705_3d, bridge_705_med_config\n",
    "from model.response import ResponseType\n",
    "from util import print_i, resize_units\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loaded vehicle data from /Users/jeremy/cs/bridge-dss/data/a16-data/a16.csv in 0.07s\n",
      "WARN: Vehicle PDF sums to 99.5, adjusted to sum to 1\n"
     ]
    }
   ],
   "source": [
    "c = bridge_705_med_config(bridge_705_3d)\n",
    "original_c = c\n",
    "# Set the directory of where to save/load responses.\n",
    "c.root_generated_data_dir = os.path.join(\"/Users/jeremy/Desktop/mesh-med-600\", c.root_generated_data_dir)\n",
    "point_a = Point(x=34.955, y=0, z=29.226 - 16.6)\n",
    "sensor_point = Point(x=21, y=0, z=-8.4)  # Sensor point to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response type aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short aliases for response types.\n",
    "rt_y = ResponseType.YTranslation\n",
    "rt_s = ResponseType.Strain\n",
    "\n",
    "# Create functions to resize, and unit strings, for each response type.\n",
    "resize_y, units_y = resize_units(rt_y.units())\n",
    "resize_s, _ = resize_units(rt_s.units())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic\n",
    "24 minutes of traffic data for damage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mins = 24\n",
    "total_seconds = total_mins * 60\n",
    "traffic_scenario = normal_traffic(c=c, lam=5, min_d=2)\n",
    "traffic_sequence, traffic, traffic_array = load_traffic(\n",
    "    c=c,\n",
    "    traffic_scenario=traffic_scenario,\n",
    "    max_time=total_seconds,\n",
    ")\n",
    "traffic_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting traffic responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.parallel_ulm = False\n",
    "damage_scenarios = [healthy_damage, pier_disp_damage([(5, 1 / 1000)])]\n",
    "damage_names = [\"Healthy\", \"Pier 5 settlement by 1mm\"]\n",
    "response_types = [rt_y, rt_s]\n",
    "responses = [[[] for _ in response_types] for _ in damage_scenarios]\n",
    "for d_i, damage_scenario in enumerate(damage_scenarios):\n",
    "    for r_i, response_type in enumerate(response_types):\n",
    "        responses[d_i][r_i] = responses_to_traffic_array(\n",
    "            c=damage_scenario.use(c)[0],\n",
    "            traffic_array=traffic_array,\n",
    "            response_type=response_type,\n",
    "            damage_scenario=damage_scenario,\n",
    "            points=[sensor_point],\n",
    "        ).T[0]  # Responses from a single point.\n",
    "responses = np.array(responses)\n",
    "responses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 minute of traffic responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h_48m, s_h_48m = deepcopy(responses[0])  # Healthy bridge. Vertical translation and strain.\n",
    "y_p_48m, s_p_48m = deepcopy(responses[1])  # Pier settlement. Vertical translation and strain.\n",
    "# Cut to 1 minute.\n",
    "y_h_1m, s_h_1m = y_h_48m[:int(len(y_h_48m) / total_mins)], s_h_48m[:int(len(s_h_48m) / total_mins)]\n",
    "y_p_1m, s_p_1m = y_p_48m[:int(len(y_p_48m) / total_mins)], s_p_48m[:int(len(s_p_48m) / total_mins)]\n",
    "\n",
    "plt.subplot(2, 2, 1) \n",
    "plt.plot(np.arange(len(y_h_1m)) * c.sensor_hz, resize_y(y_h_1m), label=\"Healthy\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"Time (seconds)\")\n",
    "plt.title(f\"1 minute of {rt_y.name()} from traffic\\nat x = {np.around(sensor_point.x, 2)} m, z = {np.around(sensor_point.z, 2)} m\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(np.arange(len(s_h_1m)) * c.sensor_hz, resize_s(s_h_1m), label=\"Healthy\")\n",
    "plt.ylabel(f\"{rt_s.name()}\")\n",
    "plt.xlabel(f\"Time (seconds)\")\n",
    "plt.title(f\"1 minute of {rt_s.name()} from traffic\\nat x = {np.around(sensor_point.x, 2)} m, z = {np.around(sensor_point.z, 2)} m\")\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(np.arange(len(y_p_1m)) * c.sensor_hz, resize_y(y_p_1m), label=\"Settlement of pier 5 by 1mm\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(np.arange(len(s_p_1m)) * c.sensor_hz, resize_s(s_p_1m), label=\"Settlement of pier 5 by 1mm\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "print(pearsonr(y_h_1m, s_h_1m))\n",
    "print(pearsonr(y_p_1m, s_p_1m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of temperature effect\n",
    "It is also useful to get an idea of what the effect of temperature is like. The responses due to temperature are clearly greater than the traffic responses.  The vertical translation and strain responses due to temperature are entirely correlated, this will possible change once the temperature model modifies the linear temperature component throughout the day, meeting this week to get an idea of approximate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# July #\n",
    "########\n",
    "\n",
    "temp_effect_y_48m = temperature.get_effect(\n",
    "    c=c,\n",
    "    response_type=rt_y,\n",
    "    points=[sensor_point],\n",
    "    temps=temps_month[\"temp\"],  # All of July.\n",
    "    responses=[y_h_48m],\n",
    "    # Traffic data is sampled per minute. Here sped up by x60.\n",
    "    # Since the given data is 48 minutes of traffic, this\n",
    "    # results in 48 hours worth of temperature data taking\n",
    "    # place over 48 minutes.\n",
    "    speed_up=60,\n",
    ")[0]  # Responses from a single point.\n",
    "\n",
    "temp_effect_s_48m = temperature.get_effect(\n",
    "    c=c,\n",
    "    response_type=rt_s,\n",
    "    points=[sensor_point],\n",
    "    temps=temps_month[\"temp\"],\n",
    "    responses=[y_h_48m],\n",
    "    speed_up=60,\n",
    ")[0]  # Responses from a single point.\n",
    "\n",
    "############\n",
    "# December #\n",
    "############\n",
    "\n",
    "from_ = datetime.fromisoformat(f\"2019-12-29T23:58\")\n",
    "to = datetime.fromisoformat(f\"2019-12-31T23:59\")\n",
    "temps_days_2 = temperature.from_to_mins(temps_year, from_, to)\n",
    "\n",
    "temp_effect_y_2_48m = temperature.get_effect(\n",
    "    c=c,\n",
    "    response_type=rt_y,\n",
    "    points=[sensor_point],\n",
    "    temps=temps_days_2[\"temp\"],\n",
    "    responses=[y_h_48m],\n",
    "    # Traffic data is sampled per minute. Here sped up by x60.\n",
    "    # Since the given data is 48 minutes of traffic, this\n",
    "    # results in 48 hours of temperature data.\n",
    "    speed_up=60,\n",
    ")[0]  # Responses from a single point.\n",
    "\n",
    "temp_effect_s_2_48m = temperature.get_effect(\n",
    "    c=c,\n",
    "    response_type=rt_s,\n",
    "    points=[sensor_point],\n",
    "    temps=temps_days_2[\"temp\"],\n",
    "    responses=[y_h_48m],\n",
    "    speed_up=60,\n",
    ")[0]  # Responses from a single point.\n",
    "\n",
    "temp_effect_y_2_48m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(responses[0][0])) * c.sensor_hz / 60\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, resize_y(temp_effect_y_48m), label=\"July 1 - 2\")\n",
    "plt.plot(x, resize_y(temp_effect_y_2_48m), label=\"December 30 - 31\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(f\"{rt_y.name()} from temperature\\nat x = {np.around(sensor_point.x, 2)} m, z = {np.around(sensor_point.z, 2)} m\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, resize_s(temp_effect_s_48m), label=\"July 1 - 2\")\n",
    "plt.plot(x, resize_s(temp_effect_s_2_48m), label=\"December 30 - 31\")\n",
    "plt.ylabel(f\"{rt_s.name()}\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(f\"{rt_s.name()} from temperature\\nat x = {np.around(sensor_point.x, 2)} m, z = {np.around(sensor_point.z, 2)} m\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "pearsonr(temp_effect_y_48m, temp_effect_s_48m)\n",
    "plt.savefig(c.get_image_path(\"classify/detection\", \"temperature-difference.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering without temperature effect\n",
    "A previous image showed how the responses change between healthy and pier settlement scenarios. Here we attempt clustering of that data without any temperature noise, to show what is possible if noise were perfectly removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_label(data, damage_scenario_label, labels):\n",
    "    \"\"\"Filter data for a given damage scenario label.\"\"\"\n",
    "    return np.array([\n",
    "        data[i] for i in range(len(data))\n",
    "        if labels[i] == damage_scenario_label\n",
    "    ])\n",
    "\n",
    "\n",
    "def colour(damage_label):\n",
    "    \"\"\"Deterministic mapping of integer to colour.\"\"\"\n",
    "    return {\n",
    "        0: \"tab:blue\",\n",
    "        1: \"tab:orange\",\n",
    "        2: \"tab:green\",\n",
    "        3: \"tab:red\",\n",
    "    }[damage_label]\n",
    "\n",
    "\n",
    "def plot_boundary(model):\n",
    "    \"\"\"Plot the decision boundary of a given model.\"\"\"\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    f = np.array(list(zip(yy.ravel(), xx.ravel())))\n",
    "    plt.tricontourf(f[:, 1], f[:, 0], model.predict(f), zorder=0)\n",
    "    \n",
    "\n",
    "def remove_outliers(model_f, data, other_data=None, good=1):\n",
    "    \"\"\"Data with outliers removed. Data must be of shape n samples x f features.\"\"\"\n",
    "    model = model_f().fit(data)\n",
    "    pred = model.predict(data)\n",
    "    if other_data is None:\n",
    "        return data[pred == good]\n",
    "    return data[pred == good], other_data[pred == good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First collect and label the responses.\n",
    "damage_features = []  # Dimensions: ((damage scenarios x samples per simulation) x features).\n",
    "damage_labels = []  # A list of length (damage scenarios x samples per simulation), each value is the damage scenario index.\n",
    "for damage_i in range(len(responses)):\n",
    "    y, s = responses[damage_i]\n",
    "    y, s = resize_y(y), resize_s(s)\n",
    "    for response_i in range(len(y)):\n",
    "        damage_features.append([y[response_i], s[response_i]])\n",
    "        damage_labels.append(damage_i)\n",
    "damage_features = np.array(damage_features)\n",
    "damage_labels = np.array(damage_labels)\n",
    "\n",
    "print(damage_features.shape, damage_labels.shape)\n",
    "# Downsample from 100 to 1 Hz.\n",
    "downsample = int(1 / c.sensor_hz)\n",
    "damage_features, damage_labels = damage_features[::downsample], damage_labels[::downsample]\n",
    "print(damage_features.shape, damage_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "plt.portrait()  # Switch from landscape to portrait plots.\n",
    "\n",
    "# Plot the reference data.\n",
    "plt.subplot(3, 1, 1)\n",
    "for damage_label in set(damage_labels):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], label=damage_names[damage_label], s=3)\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "plt.title(\"Samples for healthy and pier settled bridge\")\n",
    "legend_marker_size(plt.legend(), 50)\n",
    "\n",
    "# Choose a model.\n",
    "model_f = EllipticEnvelope\n",
    "model_f = IsolationForest\n",
    "\n",
    "# Fit the model to data and predict.\n",
    "model = model_f().fit(filter_for_label(damage_features, 0, damage_labels))\n",
    "pred = model.predict(damage_features)\n",
    "print(set(pred))\n",
    "\n",
    "# Determine accuracy of the model.\n",
    "total = defaultdict(lambda: 0)\n",
    "correct = defaultdict(lambda: 0)\n",
    "for i, damage_label in enumerate(damage_labels):\n",
    "    total[damage_label] += 1\n",
    "    if pred[i] == -1:\n",
    "        correct[damage_label] += 1\n",
    "        \n",
    "# Print accuracy of the model.\n",
    "for k, t in total.items():\n",
    "    print_i(f\"k = {k}: {correct[k]} / {t} = {correct[k] / t}\")\n",
    "\n",
    "# Plot the model results.\n",
    "plt.subplot(3, 1, 2)\n",
    "for damage_label in set(damage_labels):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plot_pred = filter_for_label(pred, damage_label, damage_labels)\n",
    "    cs = [colour(damage_label) if (damage_label == 0 and p == 1) or (damage_label > 0 and p == -1) else \"black\" for p in plot_pred]\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], c=cs, label=f\"{damage_names[damage_label]} = {correct[damage_label] / total[damage_label]:.3f}\", s=3)\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "plt.title(\"Damage probability\")\n",
    "legend = plt.legend()\n",
    "legend_marker_size(legend, 50)\n",
    "legend.legendHandles[0].set_color(\"tab:blue\")\n",
    "plot_boundary(model)\n",
    "\n",
    "#########################\n",
    "# Now without outliers! #\n",
    "#########################\n",
    "\n",
    "# Remove the outliers.\n",
    "new_damage_features, new_damage_labels = [], []\n",
    "for damage_label in set(damage_labels):\n",
    "    print(f\"Removing outliers for damage {damage_label}\")\n",
    "    df, dl = remove_outliers(\n",
    "        model_f,\n",
    "        filter_for_label(damage_features, damage_label, damage_labels),\n",
    "        filter_for_label(damage_labels, damage_label, damage_labels)\n",
    "    )\n",
    "    new_damage_features.append(df), new_damage_labels.append(dl)\n",
    "damage_features = np.concatenate(new_damage_features)\n",
    "damage_labels = np.concatenate(new_damage_labels)\n",
    "\n",
    "# Predict on the new features without outliers.\n",
    "pred = model.predict(damage_features)\n",
    "\n",
    "# Determine accuracy of the model.\n",
    "total = defaultdict(lambda: 0)\n",
    "correct = defaultdict(lambda: 0)\n",
    "for i, damage_label in enumerate(damage_labels):\n",
    "    total[damage_label] += 1\n",
    "    if pred[i] == -1:\n",
    "        correct[damage_label] += 1\n",
    "\n",
    "# Print accuracy of the model.\n",
    "for k, t in total.items():\n",
    "    print_i(f\"k = {k}: {correct[k]} / {t} = {correct[k] / t}\")\n",
    "    \n",
    "# Plot the model results.\n",
    "plt.subplot(3, 1, 3)\n",
    "for damage_label in set(damage_labels):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plot_pred = filter_for_label(pred, damage_label, damage_labels)\n",
    "    cs = [colour(damage_label) if (damage_label == 0 and p == 1) or (damage_label > 0 and p == -1) else \"black\" for p in plot_pred]\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], c=cs, label=f\"{damage_names[damage_label]} = {correct[damage_label] / total[damage_label]:.3f}\", s=3)\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "plt.title(\"Damage probability after outlier removal\")\n",
    "legend_marker_size(plt.legend(), 50)\n",
    "    \n",
    "equal_lims(\"x\", rows=3, cols=1)\n",
    "equal_lims(\"y\", rows=3, cols=1)\n",
    "plot_boundary(model)\n",
    "plt.tight_layout()\n",
    "plt.savefig(c.get_image_path(\"classify/detection\", \"healthy-and-pier-comparison.pdf\"))\n",
    "plt.landscape()  # Keep as default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation data with temperature effect\n",
    "Here we combine each of the temperature effects (July and December) and each of the damage scenarios (healthy and settled pier), to get an idea of how the signal changes (both vertical translation and strain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h_48m, s_h_48m = deepcopy(responses[0])  # Healthy bridge. Vertical translation and strain.\n",
    "y_p_48m, s_p_48m = deepcopy(responses[1])  # Pier settlement. Vertical translation and strain.\n",
    "\n",
    "plt.portrait()\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.scatter(x, resize_y(y_h_48m + temp_effect_y_48m), s=1, label=\"total effect\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(\"Healthy bridge\\nin July\")\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.scatter(x, resize_y(y_h_48m + temp_effect_y_2_48m), s=1, label=\"total effect\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(\"Healthy bridge\\nin December\")\n",
    "plt.subplot(4, 2, 5)\n",
    "plt.scatter(x, resize_y(y_p_48m + temp_effect_y_48m), s=1, label=\"total effect\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(\"Pier settled bridge\\nin July\")\n",
    "plt.subplot(4, 2, 7)\n",
    "plt.scatter(x, resize_y(y_p_48m + temp_effect_y_2_48m), s=1, label=\"total effect\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(\"Pier settled bridge\\nin December\")\n",
    "equal_lims(\"y\", 4, 2, [1, 3, 5, 7])\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "plt.scatter(x, resize_s(s_h_48m + temp_effect_s_48m), s=1, label=\"total effect\", c=\"tab:orange\")\n",
    "plt.ylabel(f\"{rt_s.name()}\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(\"Healthy bridge\\nin July\")\n",
    "plt.subplot(4, 2, 4)\n",
    "plt.scatter(x, resize_s(s_h_48m + temp_effect_s_2_48m), s=1, label=\"total effect\", c=\"tab:orange\")\n",
    "plt.ylabel(f\"{rt_s.name()}\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(\"Healthy bridge\\nin December\")\n",
    "plt.subplot(4, 2, 6)\n",
    "plt.scatter(x, resize_s(s_p_48m + temp_effect_s_48m), s=1, label=\"total effect\", c=\"tab:orange\")\n",
    "plt.ylabel(f\"{rt_s.name()}\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(\"Pier settled bridge\\nin July\")\n",
    "plt.subplot(4, 2, 8)\n",
    "plt.scatter(x, resize_s(s_p_48m + temp_effect_s_2_48m), s=1, label=\"total effect\", c=\"tab:orange\")\n",
    "plt.ylabel(f\"{rt_s.name()}\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.title(\"Pier settled bridge\\nin December\")\n",
    "equal_lims(\"y\", 4, 2, [2, 4, 6, 8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.landscape()\n",
    "plt.savefig(c.get_image_path(\"classify/detection\", \"temp-and-damage.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(temp_effect_y_48m)\n",
    "temp_effect_y_48m = savgol_filter(deepcopy(temp_effect_y_48m), 12001, 4) # window size 51, polynomial order 3\n",
    "temp_effect_y_2_48m = savgol_filter(deepcopy(temp_effect_y_2_48m), 12001, 4) # window size 51, polynomial order 3\n",
    "temp_effect_s_48m = savgol_filter(deepcopy(temp_effect_s_48m), 12001, 4) # window size 51, polynomial order 3\n",
    "temp_effect_s_2_48m = savgol_filter(deepcopy(temp_effect_s_2_48m), 12001, 4) # window size 51, polynomial order 3\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(temp_effect_y_48m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with temperature effect\n",
    "Clustering the first minute of data from each of the four scenarios above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_names = [\n",
    "    (y_h_48m + temp_effect_y_48m, s_h_48m + temp_effect_s_48m, \"Healthy bridge in July\"),\n",
    "    (y_h_48m + temp_effect_y_2_48m, s_h_48m + temp_effect_s_2_48m, \"Healthy bridge in December\"),\n",
    "    (y_p_48m + temp_effect_y_48m, s_p_48m + temp_effect_s_48m, \"Pier settled bridge in July\"),\n",
    "    (y_p_48m + temp_effect_y_2_48m, s_p_48m + temp_effect_s_2_48m, \"Pier settled bridge in December\"),\n",
    "]\n",
    "damage_names = list(map(lambda x: x[2], data_and_names))\n",
    "\n",
    "# First collect and label the responses.\n",
    "damage_features = []  # Dimensions: ((scenarios x samples per simulation) x features).\n",
    "damage_labels = []  # A list of length (scenarios x samples per simulation), each value is the damage scenario index.\n",
    "for label_i, (y, s, _name) in enumerate(data_and_names):\n",
    "    y, s = resize_y(deepcopy(y)), resize_s(deepcopy(s))\n",
    "    for response_i in range(len(y)):\n",
    "        damage_features.append([y[response_i], s[response_i]])\n",
    "        damage_labels.append(label_i)\n",
    "damage_features = np.array(damage_features)\n",
    "damage_labels = np.array(damage_labels)\n",
    "\n",
    "# Downsample from 100 to 1 Hz.\n",
    "downsample = int(1 / c.sensor_hz)\n",
    "damage_features, damage_labels = damage_features[::downsample], damage_labels[::downsample]\n",
    "\n",
    "print(damage_features.shape)\n",
    "print(damage_labels.shape)\n",
    "print(set(damage_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.portrait()\n",
    "\n",
    "# Plot the reference data.\n",
    "plt.subplot(3, 1, 1)\n",
    "for damage_label in set(damage_labels):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], label=damage_names[damage_label], s=1)\n",
    "plt.title(\"Raw samples for four scenarios\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "legend = plt.legend()\n",
    "for damage_label in set(damage_labels):\n",
    "    legend.legendHandles[damage_label].set_color(colour(damage_label))\n",
    "legend_marker_size(legend, 50)\n",
    "\n",
    "# Calculate labels with a model.\n",
    "model = model_f().fit(filter_for_label(damage_features, 0, damage_labels))\n",
    "pred = model.predict(damage_features)\n",
    "print(set(pred))\n",
    "\n",
    "# Determine accuracy of the model.\n",
    "total = defaultdict(lambda: 0)\n",
    "correct = defaultdict(lambda: 0)\n",
    "healthy_labels = [0, 1]\n",
    "for i, damage_label in enumerate(damage_labels):\n",
    "    total[damage_label] += 1\n",
    "    if pred[i] == -1:\n",
    "        correct[damage_label] += 1\n",
    "        \n",
    "# Print accuracy of the model.\n",
    "for k, t in total.items():\n",
    "    print_i(f\"k = {k}: {correct[k]} / {t} = {correct[k] / t}\")\n",
    "    \n",
    "# Plot the damage detection results.\n",
    "plt.subplot(3, 1, 2)\n",
    "for damage_label in set(damage_labels):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plot_pred = filter_for_label(pred, damage_label, damage_labels)\n",
    "    cs = [colour(damage_label) if (damage_label in healthy_labels and p == 1) or (damage_label not in healthy_labels and p == -1) else \"black\" for p in plot_pred]\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], c=cs, label=f\"{damage_names[damage_label]} = {correct[damage_label] / total[damage_label]:.2f}\", s=2)\n",
    "plt.title(\"Damage probability\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "plt.legend()\n",
    "plot_boundary(model)\n",
    "legend = plt.legend()\n",
    "for damage_label in set(damage_labels):\n",
    "    legend.legendHandles[damage_label].set_color(colour(damage_label))\n",
    "legend_marker_size(legend, 50)\n",
    "\n",
    "#########################\n",
    "# Now without outliers! #\n",
    "#########################\n",
    "\n",
    "# Remove the outliers.\n",
    "new_damage_features, new_damage_labels = [], []\n",
    "for damage_label in set(damage_labels):\n",
    "    print(f\"Removing outliers for damage {damage_label}\")\n",
    "    df, dl = remove_outliers(\n",
    "        model_f,\n",
    "        filter_for_label(damage_features, damage_label, damage_labels),\n",
    "        filter_for_label(damage_labels, damage_label, damage_labels)\n",
    "    )\n",
    "    new_damage_features.append(df), new_damage_labels.append(dl)\n",
    "damage_features = np.concatenate(new_damage_features)\n",
    "damage_labels = np.concatenate(new_damage_labels)\n",
    "\n",
    "# Predict on the new features without outliers.\n",
    "pred = model.predict(damage_features)\n",
    "\n",
    "# Determine accuracy of the model.\n",
    "total = defaultdict(lambda: 0)\n",
    "correct = defaultdict(lambda: 0)\n",
    "for i, damage_label in enumerate(damage_labels):\n",
    "    total[damage_label] += 1\n",
    "    if pred[i] == -1:\n",
    "        correct[damage_label] += 1\n",
    "\n",
    "# Print accuracy of the model.\n",
    "for k, t in total.items():\n",
    "    print_i(f\"k = {k}: {correct[k]} / {t} = {correct[k] / t}\")\n",
    "    \n",
    "# Plot the model results.\n",
    "plt.subplot(3, 1, 3)\n",
    "for damage_label in set(damage_labels):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plot_pred = filter_for_label(pred, damage_label, damage_labels)\n",
    "    cs = [colour(damage_label) if (damage_label in healthy_labels and p == 1) or (damage_label not in healthy_labels and p == -1) else \"black\" for p in plot_pred]\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], c=cs, label=f\"{damage_names[damage_label]} = {correct[damage_label] / total[damage_label]:.3f}\", s=3)\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "plt.title(\"Damage probability after outlier removal\")\n",
    "legend = plt.legend()\n",
    "for damage_label in set(damage_labels):\n",
    "    legend.legendHandles[damage_label].set_color(colour(damage_label))\n",
    "legend_marker_size(legend, 50)\n",
    "\n",
    "equal_lims(\"x\", rows=3, cols=1)\n",
    "equal_lims(\"y\", rows=3, cols=1)\n",
    "plot_boundary(model)\n",
    "plt.tight_layout()\n",
    "plt.savefig(c.get_image_path(\"classify/detection\", \"healthy-and-pier-comparison.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of noise.\n",
    "From the previous plot, it is clear that the healthy bridge in December is misclassified as being in a damaged state, based on the model which was trained on healthy data in July. Therefore noise removal is clearly necessary, in order to show that the healthy bridge in December can be classified as healthy. We start with an experiment on the removal of daily temperature effect. Daily and annual temperature are on the same order of magnitude, therefore simply ignoring the short-term effect is not good enough, as different days can have very different daily temperature profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy.polynomial.polynomial as poly\n",
    "#     y_samples = list(map(lambda i: signal[int(i)], indices))\n",
    "#     coefs = poly.polyfit(indices, y_samples, 3)\n",
    "#     fit = poly.polyval(np.arange(0, len(signal)), coefs)\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "def remove_temp(num_samples, signal, ref=None):\n",
    "    \"\"\"Data with outliers removed. Data must be of shape n samples x f features.\"\"\"\n",
    "    indices = list(map(int, np.linspace(0, len(signal) - 1, num_samples)))\n",
    "    y_samples = []\n",
    "    for i_lo, i_hi in zip(indices[:-1], indices[1:]):\n",
    "        y_samples.append(np.mean(signal[i_lo:i_hi]))\n",
    "    n =  np.interp(np.arange(len(signal)), indices[1:], y_samples)\n",
    "    if ref is not None:\n",
    "        error = deepcopy(signal) - n - ref\n",
    "        return deepcopy(signal) - n, error\n",
    "    return deepcopy(signal) - n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h_48m, s_h_48m = deepcopy(responses[0])  # Healthy bridge. Vertical translation and strain.\n",
    "# y_p_48m, s_p_48m = deepcopy(responses[1])  # Pier settlement. Vertical translation and strain.\n",
    "len_24m = int(len(y_h_48m) / 2)\n",
    "y_h_24m, s_h_24m = y_h_48m[:len_24m], s_h_48m[:len_24m]\n",
    "# y_p_24m, s_p_24m = y_p_48m[:len_24m], s_p_48m[:len_24m]\n",
    "\n",
    "# List of tuple (y translation time series, strain time series, name string).\n",
    "# NOTE: these 24m time series are downsampled to from 100 to 1 Hz.\n",
    "data = []\n",
    "damage_names = []\n",
    "\n",
    "# TODO: Add other months!\n",
    "for day_i in range(1, 30 + 1):\n",
    "    from_ = datetime.fromisoformat(f\"2019-07-{('0' if day_i < 10 else '')  + str(day_i)}T00:00\")\n",
    "    to = datetime.fromisoformat(f\"2019-07-{('0' if day_i < 9 else '') + str(day_i + 1)}T00:00\")\n",
    "    temps_day = temperature.from_to_mins(temps_year, from_, to)\n",
    "    y_day = temperature.get_effect(c=c, response_type=rt_y, points=[point_a], temps=temps_day[\"temp\"], responses=[y_h_24m], speed_up=60)[0]\n",
    "    s_day = temperature.get_effect(c=c, response_type=rt_s, points=[point_a], temps=temps_day[\"temp\"], responses=[y_h_24m], speed_up=60)[0]\n",
    "    data.append((y_h_24m[::downsample], (y_h_24m + y_day)[::downsample], s_h_24m[::downsample], (s_h_24m + s_day)[::downsample]))\n",
    "    damage_names.append(day_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the mean error against variance, doesn't show much!\n",
    "y_error, y_var, s_error, s_var = [], [], [], []\n",
    "for y_ref, y_temp, s_ref, s_temp in data:\n",
    "    y_removed_temp, y_err = remove_temp(24, y_temp, ref=y_ref)\n",
    "    s_removed_temp, s_err = remove_temp(24, s_temp, ref=s_ref)\n",
    "    y_error.append(np.mean(y_err))\n",
    "    s_error.append(s_err)\n",
    "    y_var.append(np.var(y_temp))\n",
    "# plt.scatter(y_error, y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.landscape()\n",
    "ref = data[0][0]\n",
    "signal = data[0][1]\n",
    "removed_temp, error = remove_temp(24, signal, ref=ref)\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(ref)\n",
    "plt.title(\"Data without daily temperature effect\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(signal)\n",
    "plt.title(\"Data with daily temperature effect\")\n",
    "plt.subplot(3, 1, 3)\n",
    "# plt.plot(removed_temp - error, color=\"red\")\n",
    "plt.plot(removed_temp, label=\"After removal\")\n",
    "plt.plot(error, label=f\"Mean error = {np.mean(error):.5f}\")\n",
    "plt.title(\"Removal of daily temperature effect\")\n",
    "plt.legend()\n",
    "equal_lims(\"y\", 3, 1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.portrait()\n",
    "\n",
    "# First collect and label the responses.\n",
    "damage_features = []  # Dimensions: ((scenarios x samples per simulation) x features).\n",
    "damage_labels = []  # A list of length (scenarios x samples per simulation), each value is the damage scenario index.\n",
    "data_ = data[:3]  # TODO: Remove shortening of list!\n",
    "for label_i, (y_ref, y_temp, s_ref, s_temp) in enumerate(list(data_)):\n",
    "    y, s = resize_y(deepcopy(y_temp)), resize_s(deepcopy(s_temp))\n",
    "    plt.subplot(len(data_), 1, label_i + 1)\n",
    "    plt.plot(s)\n",
    "    y = remove_outliers(model_f, remove_temp(24, y).reshape(-1, 1))\n",
    "    s = remove_outliers(model_f, remove_temp(24, s).reshape(-1, 1))\n",
    "    plt.plot(s)\n",
    "#     y, s = remove_temp(24, y), remove_temp(48, s)\n",
    "#     plt.subplot(len(data_and_names) * 2, 1, 1 + label_i * 2)\n",
    "#     plt.plot(y)\n",
    "#     print(y.shape, y_new.shape)\n",
    "#     plt.subplot(len(data_and_names), 1, 1 + label_i)\n",
    "#     plt.plot(y)\n",
    "#     plt.plot(y - y_new, label=\"new\")\n",
    "#     print(y.shape)\n",
    "#     print(r.shape)\n",
    "#     plt.plot(y_r)\n",
    "    for response_i in range(len(y)):\n",
    "        damage_features.append([y[response_i], s[response_i]])\n",
    "        damage_labels.append(label_i)\n",
    "damage_features = np.array(damage_features)\n",
    "damage_labels = np.array(damage_labels)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: long-term damage (pier settlement)\n",
    "Here we run N simulations, combining removal techniques (daily & annual), and show how detection accuracy is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(damage_features.shape)\n",
    "print(damage_labels.shape)\n",
    "print(set(damage_labels))\n",
    "\n",
    "# Plot the reference data.\n",
    "plt.subplot(3, 1, 1)\n",
    "for damage_label in sorted(set(damage_labels)):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], label=damage_names[damage_label], s=1)\n",
    "plt.title(\"Raw samples for four scenarios\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "legend = plt.legend()\n",
    "for damage_label in set(damage_labels):\n",
    "    legend.legendHandles[damage_label].set_color(colour(damage_label))\n",
    "legend_marker_size(legend, 50)\n",
    "\n",
    "# Calculate labels with a model.\n",
    "model = model_f().fit(filter_for_label(damage_features, 0, damage_labels))\n",
    "pred = model.predict(damage_features)\n",
    "print(set(pred))\n",
    "\n",
    "# Determine accuracy of the model.\n",
    "total = defaultdict(lambda: 0)\n",
    "correct = defaultdict(lambda: 0)\n",
    "healthy_labels = [0, 1]\n",
    "for i, damage_label in enumerate(damage_labels):\n",
    "    total[damage_label] += 1\n",
    "    if pred[i] == -1:\n",
    "        correct[damage_label] += 1\n",
    "\n",
    "# Print accuracy of the model.\n",
    "for k, t in total.items():\n",
    "    print_i(f\"k = {k}: {correct[k]} / {t} = {correct[k] / t}\")\n",
    "    \n",
    "# Plot the damage detection results.\n",
    "plt.subplot(3, 1, 2)\n",
    "for damage_label in set(damage_labels):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plot_pred = filter_for_label(pred, damage_label, damage_labels)\n",
    "    cs = [colour(damage_label) if (damage_label in healthy_labels and p == 1) or (damage_label not in healthy_labels and p == -1) else \"black\" for p in plot_pred]\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], c=cs, label=f\"{damage_names[damage_label]} = {correct[damage_label] / total[damage_label]:.2f}\", s=2)\n",
    "plt.title(\"Damage probability\")\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "plt.legend()\n",
    "plot_boundary(model)\n",
    "# legend = plt.legend()\n",
    "# for damage_label in set(damage_labels):\n",
    "#     legend.legendHandles[damage_label].set_color(colour(damage_label))\n",
    "# legend_marker_size(legend, 50)\n",
    "\n",
    "#########################\n",
    "# Now without outliers! #\n",
    "#########################\n",
    "\n",
    "# Remove the outliers.\n",
    "new_damage_features, new_damage_labels = [], []\n",
    "for damage_label in set(damage_labels):\n",
    "    print(f\"Removing outliers for damage {damage_label}\")\n",
    "    df, dl = remove_outliers(\n",
    "        model_f,\n",
    "        filter_for_label(damage_features, damage_label, damage_labels),\n",
    "        filter_for_label(damage_labels, damage_label, damage_labels)\n",
    "    )\n",
    "    new_damage_features.append(df), new_damage_labels.append(dl)\n",
    "damage_features = np.concatenate(new_damage_features)\n",
    "damage_labels = np.concatenate(new_damage_labels)\n",
    "\n",
    "# Predict on the new features without outliers.\n",
    "pred = model.predict(damage_features)\n",
    "\n",
    "# Determine accuracy of the model.\n",
    "total = defaultdict(lambda: 0)\n",
    "correct = defaultdict(lambda: 0)\n",
    "for i, damage_label in enumerate(damage_labels):\n",
    "    total[damage_label] += 1\n",
    "    if pred[i] == -1:\n",
    "        correct[damage_label] += 1\n",
    "\n",
    "# Print accuracy of the model.\n",
    "for k, t in total.items():\n",
    "    print_i(f\"k = {k}: {correct[k]} / {t} = {correct[k] / t}\")\n",
    "    \n",
    "# Plot the model results.\n",
    "plt.subplot(3, 1, 3)\n",
    "for damage_label in set(damage_labels):\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plot_pred = filter_for_label(pred, damage_label, damage_labels)\n",
    "    cs = [colour(damage_label) if (damage_label in healthy_labels and p == 1) or (damage_label not in healthy_labels and p == -1) else \"black\" for p in plot_pred]\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], c=cs, label=f\"{damage_names[damage_label]} = {correct[damage_label] / total[damage_label]:.3f}\", s=3)\n",
    "plt.ylabel(f\"{rt_y.name()} ({units_y})\")\n",
    "plt.xlabel(f\"{rt_s.name()}\")\n",
    "plt.title(\"Damage probability after outlier removal\")\n",
    "legend = plt.legend()\n",
    "for damage_label in set(damage_labels):\n",
    "    legend.legendHandles[damage_label].set_color(colour(damage_label))\n",
    "legend_marker_size(legend, 50)\n",
    "\n",
    "equal_lims(\"x\", rows=3, cols=1)\n",
    "equal_lims(\"y\", rows=3, cols=1)\n",
    "plot_boundary(model)\n",
    "plt.tight_layout()\n",
    "plt.savefig(c.get_image_path(\"classify/detection\", \"healthy-and-pier-comparison.pdf\"))\n",
    "\n",
    "plt.landscape()  # Keep as default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of distance from pier.\n",
    "\n",
    "The distance a sensor is from a pier will affect the accuracy of damage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot.geometry import top_view_bridge\n",
    "from util import flatten\n",
    "\n",
    "pier_centers = [Point(x=pier.x, y=0, z=pier.z) for pier in c.bridge.supports]\n",
    "\n",
    "\n",
    "def nearest_pier(point):\n",
    "    \"\"\"Index of nearest pier for a given point.\"\"\"\n",
    "    distance, nearest = np.inf, 0\n",
    "    for pier_i, pier_point in enumerate(pier_centers):\n",
    "        if point.distance(pier_point) < distance:\n",
    "            distance = point.distance(pier_point)\n",
    "            nearest = pier_i\n",
    "    return nearest\n",
    "\n",
    "\n",
    "# Number of sensors in X and Z directions.\n",
    "X, Z = 200, 60\n",
    "# X, Z = 100, 10\n",
    "width, height = c.bridge.length / X, c.bridge.width / Z\n",
    "\n",
    "# Sensors, as a grid.\n",
    "grid_sensors = [\n",
    "    [Point(x=x, y=0, z=z) for z in np.linspace(c.bridge.z_min + height / 2, c.bridge.z_max - height / 2, num=Z)]\n",
    "    for x in np.linspace(c.bridge.x_min + width / 2, c.bridge.x_max - width / 2, num=X)\n",
    "]\n",
    "# Sensors, as a flattened list.\n",
    "sensors = flatten(grid_sensors, Point)\n",
    "\n",
    "# Selected pier, for each sensor.\n",
    "sensor_piers = list(map(nearest_pier, sensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_view_bridge(c.bridge, compass=False, piers=True)\n",
    "cmap = mpl.cm.get_cmap(\"tab10\")\n",
    "for sensor, pier in zip(sensors, sensor_piers):\n",
    "    plt.gca().add_patch(plt.Rectangle(\n",
    "        (sensor.x - width / 2, sensor.z - height / 2),\n",
    "        width,\n",
    "        height,\n",
    "        facecolor=cmap(pier % 10),\n",
    "        edgecolor=\"black\",\n",
    "    ))\n",
    "plt.title(\"Sensors closest to each pier\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We downsample the traffic array before collecting responses\n",
    "# because the number of points at which responses are collected\n",
    "# is very lage making the calculation prohibitively expensive.\n",
    "\n",
    "downsample = int(100 / 1)\n",
    "down_traffic_array = np.array(traffic_array[::downsample])\n",
    "print(traffic_array.shape)\n",
    "print(down_traffic_array.shape)\n",
    "\n",
    "responses = [[[] for _ in response_types] for _ in damage_scenarios]\n",
    "for d_i, damage_scenario in enumerate(damage_scenarios):\n",
    "    for r_i, response_type in enumerate(response_types):\n",
    "        print(f\"Collecting responses for (damage, responses) = ({damage_scenario.name}, {response_type.name()})\")\n",
    "        responses[d_i][r_i] = responses_to_traffic_array(\n",
    "            c=c,\n",
    "            traffic_array=down_traffic_array,\n",
    "            response_type=response_type,\n",
    "            damage_scenario=damage_scenario,\n",
    "            points=sensors,\n",
    "            sim_runner=OSRunner(c),\n",
    "        ).T\n",
    "responses = np.array(responses)\n",
    "responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classify import without\n",
    "\n",
    "# Remove unavailable sensors.\n",
    "print(f\"All sensors = {len(sensors)}\")\n",
    "without_f = without.without_sensors(c=c, pier_radius=1.5, track_radius=0.5)\n",
    "classify_sensors = list(filter(lambda s: not without_f(s), sensors))\n",
    "print(f\"Available sensors = {len(classify_sensors)}\")\n",
    "\n",
    "# Limit to the sensors closes to pier 5.\n",
    "# classify_sensors = set([sensor for sensor, pier in zip(sensors, sensor_piers) if pier == 5])\n",
    "\n",
    "# Collect and label the responses.\n",
    "damage_features = defaultdict(list)  # Dimensions: (sensors : (scenarios x samples per simulation) x features).\n",
    "damage_labels = defaultdict(list)  # Dimensions: (sensors : (scenarios x samples per simulation)), each value is the damage scenario index.\n",
    "for sensor_i, sensor in enumerate(sensors):\n",
    "    if sensor in classify_sensors:\n",
    "        print(f\"Sensor {sensor_i}\", end=\"\\r\")\n",
    "        for damage_label, (y, s) in enumerate(responses):\n",
    "            assert len(sensors) == len(y)\n",
    "            y, s = resize_y(y[sensor_i]), resize_s(s[sensor_i])\n",
    "            for response_i in range(len(y)):\n",
    "                damage_features[sensor_i].append([y[response_i], s[response_i]])\n",
    "                damage_labels[sensor_i].append(damage_label)\n",
    "        damage_features[sensor_i] = np.array(damage_features[sensor_i])\n",
    "        damage_labels[sensor_i] = np.array(damage_labels[sensor_i])\n",
    "#         print(damage_features[sensor_i].shape)\n",
    "#         print(damage_labels[sensor_i].shape)\n",
    "#         print(set(damage_labels[sensor_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    acc_pier = dict()\n",
    "    acc_healthy = dict()\n",
    "    labels_pier = dict()\n",
    "    labels_healthy = dict()\n",
    "    for sensor_i, sensor in enumerate(sensors):\n",
    "        if sensor in classify_sensors:\n",
    "            healthy_features = filter_for_label(damage_features[sensor_i], 0, damage_labels[sensor_i])\n",
    "            pier_features = filter_for_label(damage_features[sensor_i], 1, damage_labels[sensor_i])\n",
    "            model = IsolationForest().fit(healthy_features)  # Train on healthy data.\n",
    "            labels_healthy[sensor_i] = model.predict(healthy_features)  # Test on healthy data.\n",
    "            labels_pier[sensor_i] = model.predict(pier_features)  # Test on pier settlement data.\n",
    "            acc_pier[sensor_i] = len([x for x in labels_pier[sensor_i] if x == 1]) / len(labels_pier[sensor_i])\n",
    "            acc_healthy[sensor_i] = len([x for x in labels_healthy[sensor_i] if x == 1]) / len(labels_healthy[sensor_i])\n",
    "    #         print(f\"Sensor {sensor_i}\", acc_pier[sensor_i], acc_healthy[sensor_i], end=\"\\r\")\n",
    "            print(f\"Sensor {sensor_i}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "# For each sensor, the euclidean distance between the mean of\n",
    "# the features under the healthy and damaged scenarios is\n",
    "# calculated.\n",
    "if False:\n",
    "    dist = dict()  # <sensor index: mean>\n",
    "    for sensor_i, sensor in enumerate(sensors):\n",
    "        if sensor in classify_sensors:\n",
    "            healthy_features = filter_for_label(damage_features[sensor_i], 0, damage_labels[sensor_i])\n",
    "            pier_features = filter_for_label(damage_features[sensor_i], 1, damage_labels[sensor_i])\n",
    "            healthy_mean = np.mean(healthy_features.T[0]), np.mean(healthy_features.T[1])\n",
    "            pier_mean = np.mean(pier_features.T[0]), np.mean(pier_features.T[1])\n",
    "            dist[sensor_i] = distance.euclidean(healthy_mean, pier_mean)\n",
    "            print(f\"Sensor {sensor_i}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = acc_pier\n",
    "# to_plot = dist\n",
    "amin, amax = min(to_plot.values()), max(to_plot.values())\n",
    "cmap = mpl.cm.get_cmap(\"jet\")\n",
    "norm = mpl.colors.Normalize(vmin=amin, vmax=amax)\n",
    "\n",
    "for sensor_i, (sensor, pier) in enumerate(zip(sensors, sensor_piers)):\n",
    "    facecolor = cmap(1 - norm(to_plot[sensor_i])) if sensor in classify_sensors else \"white\"\n",
    "    plt.gca().add_patch(plt.Rectangle(\n",
    "        (sensor.x - width / 2, sensor.z - height / 2),\n",
    "        width,\n",
    "        height,\n",
    "        facecolor=facecolor,\n",
    "#         edgecolor=\"black\",\n",
    "    ))\n",
    "top_view_bridge(c.bridge, compass=False, piers=True)\n",
    "\n",
    "plt.title(\"Sensors detecting pier settlement in the absence of noise\")\n",
    "plt.colorbar(plt.cm.ScalarMappable(cmap=cmap, norm=norm))\n",
    "plt.savefig(c.get_image_path(\"classify/detection\", \"sensors-pier-detection.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cracked concrete\n",
    "Similar to pier settlement, we first show how the responses change under healthy and damaged scenario, except this time the damage scenario is cracked concrete rather than pier settlement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classify.scenario.bridge import transverse_crack\n",
    "    \n",
    "damage_scenarios = [healthy_damage, transverse_crack()]\n",
    "sensor = Point(x=52, y=0, z=-8.4)\n",
    "\n",
    "responses = [[[] for _ in response_types] for _ in damage_scenarios]\n",
    "for d_i, damage_scenario in enumerate(damage_scenarios):\n",
    "    for r_i, response_type in enumerate(response_types):\n",
    "        print(f\"Collecting responses for (damage, responses) = ({damage_scenario.name}, {response_type.name()})\")\n",
    "        responses[d_i][r_i] = responses_to_traffic_array(\n",
    "            c=c,\n",
    "            traffic_array=down_traffic_array,\n",
    "            response_type=response_type,\n",
    "            damage_scenario=damage_scenario,\n",
    "            points=[sensor],\n",
    "            sim_runner=OSRunner(c),\n",
    "        ).T\n",
    "responses = np.array(responses)\n",
    "responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h, s_h = deepcopy(responses[0])  # Healthy bridge. Vertical translation and strain.\n",
    "y_c, s_c = deepcopy(responses[1])  # Cracked bridge. Vertical translation and strain.\n",
    "data_and_names = [\n",
    "    (y_h + temp_effect_y[::downsample], s_h, \"Healthy bridge in July\"),\n",
    "    (y_h + temp_effect_y_2[::downsample], s_h, \"Healthy bridge in December\"),\n",
    "    (y_c + temp_effect_y[::downsample], s_c, \"Cracked bridge in July\"),\n",
    "    (y_c + temp_effect_y_2[::downsample], s_c, \"Cracked bridge in December\"),\n",
    "]\n",
    "damage_names = list(map(lambda x: x[2], data_and_names))\n",
    "\n",
    "# First collect and label the responses.\n",
    "damage_features = []  # Dimensions: ((scenarios x samples per simulation) x features).\n",
    "damage_labels = []  # A list of length (scenarios x samples per simulation), each value is the damage scenario index.\n",
    "for label_i, (y, s, _name) in enumerate(data_and_names):\n",
    "    y, s = resize_y(y), resize_s(s)\n",
    "    for response_i in range(len(y)):\n",
    "        damage_features.append([y[response_i], s[response_i]])\n",
    "        damage_labels.append(label_i)\n",
    "damage_features = np.array(damage_features)\n",
    "damage_labels = np.array(damage_labels)\n",
    "\n",
    "print(damage_features.shape)\n",
    "print(damage_labels.shape)\n",
    "print(set(damage_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reference data.\n",
    "for damage_label in sorted(damage_labels)[::]:\n",
    "    print(f\"Damage label = {damage_label}\", end=\"\\r\")\n",
    "    plot_features = filter_for_label(damage_features, damage_label, damage_labels)\n",
    "    plt.scatter(plot_features[:, 1], plot_features[:, 0], label=damage_names[damage_label], s=1)\n",
    "plt.title(\"Reference data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
