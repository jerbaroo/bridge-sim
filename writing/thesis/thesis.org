#+AUTHOR: Jeremy Barisch-Rooney
#+TITLE: Utilizing sensors for the purpose of building a decision support system for bridge maintenance

* Introduction
   # Introduction of the research question/title.
   The probability of a bridge to fail increases over time until it is no longer
   considered safe for use. Maintenance of a bridge is typically carried out
   when something goes wrong or according to a preventative maintenance schedule
   based on expert knowledge, neither approach making the best use of limited
   maintenance resources. Sensors can provide useful real-time information
   without the delay or cost of a manual maintenance check. How sensors can be
   utilized to build a decision support system (DSS) for bridge maintenance is
   the topic of this thesis.

   # Sensors and why bridge 705.
   Sensors on bridges can provide real-time measurements of the responses of the
   part of the bridge on which they are installed. Depending on the sensor-type
   this measured response may be translation, rotation, vibration or one of many
   other types of response. In this thesis the focus is on a single bridge,
   bridge 705 in Amsterdam. The reason bridge 705 was chosen is because a 3D
   finite element model (FEM) is available for the bridge, and a field test was
   conducted where known loads were applied to the bridge and the corresponding
   sensor measurements recorded. The FEM is useful so that sensor measurements
   for a known load can be simulated without having to conduct a field test, the
   measurements from the field test allow us to verify the accuracy of the data
   generated by simulation.

   # A decision support system.
   A DSS for bridge maintenance must provide information on the damage status of
   the bridge to the user of the system or policy maker. Thus it is necessary to
   transform the responses measured by the sensors into a report of the damage
   condition of the bridge. To accomplish this a condition classification model
   (CCM) is built which transforms sensor measurements into a condition report.
   The CCM is based on two methods referred to from now on as abnormal condition
   classification (ACC) and similar structure similar behaviour (SSSB).

   # ACC.
   The goal of ACC is to determine if the condition of the bridge has deviated
   from the normal range of conditions. To build an ACC system it is necessary
   to first find out what the range of sensor measurements are during normal
   operation of the bridge. This is achieved by applying a normal range of
   loading conditions to the FEM and recording the simulated sensor
   measurements. Then a one-class classifier can be applied to the simulated
   responses and be used to decide if any subsequent sensor measurements fall
   within the expected normal range of responses or not.

   # SSSB.
   The SSSB method is based on the assumption that similar structures should
   behave in a similar manner when subjected to the same load. Bridge 705 in
   Amsterdam has seven spans each with the same dimensions, ignoring the small
   differences due to construction and time in operation. To develop an SSSB
   system loads must be "driven" across the bridge in the FEM, then an analysis
   must be performed on the difference between sensor measurements from sensors
   at equivalent positions on each substructure.
   
   # Thesis structure.
   The research question that this thesis answers is: how can sensors be
   utilized to build a DSS for bridge maintenance. The structure of this thesis
   and how the research question is answered is as follows. First a review of
   relevant literature and background material is presented. The DSS is then
   introduced at a high-level, showing how the separate components interact. The
   components of the DSS are examined in detail, with a large focus on the
   condition classification model that determines if sensor measurements
   represent an abnormal condition of the bridge. Since analysis is presented of
   which sensor types and what sensor placement is optimal for detecting such an
   abnormal condition. A finite element model is used to simulate sensor
   measurements in order to address the lack of available data. Due to the
   safety requirements of any bridge, uncertainty measures for the damage
   estimates are calculated. Once the capabilities and limitations of the model
   are understood, an outline of a DSS is presented for policy makers which
   includes the model and a cost-benefit analysis is presented of the system.
   Finally (stretch-goal) an investigation is conducted into how such a system
   can be generalized to bridges other than bridge 705.
   
* Literature review
  This section contains a review of relevant literature studied during this
  thesis project.
** Neural Clouds for Monitoring of Complex Systems
   # One-class classification.
   In one-class classification, a classifier attempts to identify objects of a
   single class among all objects by learning from a training set that consists
   only of objects of that class. One-class classifiers are useful in the domain
   of system condition monitoring because often only data corresponding to the
   normal range of operating conditions is available. Data corresponding to the
   class of abnormal conditions, when a failure or breakdown of a system has
   occurred, is often not available or is difficult or expensive to obtain.

   # Neural Clouds algorithm.
   The Neural Clouds (NC) method presented in TODO:REF is a one-class classifier
   which provides a confidence measure of the condition of a complex system. In
   the NC algorithm we are dealing with measurements from a real object where
   each measurement is considered as a point in n-dimensional space.

   # Normalization and clustering.
   First a normalization procedure is applied to the data to avoid clustering
   problems in the subsequent step. The data is then clustered and the centroids
   of the clusters extracted. The centroids are then encapsulated with "Gaussian
   bells", and these Gaussian bells are normalized to avoid outliers in the
   data.

   # Height = probability.
   The summation of the Gaussian bells results in a height =h= for each point
   =p= on the hyperplane of parameter values. The value of =h= at a point =p=
   can be interpreted as the probability of the parameter values at =p= falling
   within the normal conditions represented by the training data.

   # Comparison.
   In comparison to other one-class classifiers, the NC method has an advantage
   in condition monitoring in that it creates this unique plateau where height
   can be interpreted as probability of the system condition. Figure TODO:FIG
   shows this plateau in comparison with other one-class classifiers.

   # Limitations.
   It is important to note that when significant changes occur in the normal
   state of the system, perhaps due to environmental changes, then the NC
   classifier should be retrained in order to avoid a false alarm. However, if a
   NC classifier is continually being retrained with real-time data then it may
   not detect a gradual long-term change to the system.
** Combining Data-driven Methods with Finite Element Analysis for Flood Early Warning Systems
   # Introduction and why levee collapse.
   In this paper TODO:REF a system for real-time levee condition monitoring is
   presented based on a combination of data-driven methods and finite-element
   analysis. Levee monitoring allows for earlier warning signals incase of levee
   failure, compared to the current method of visual inspection. The problem
   with visual inspection is that when deformations are visiable at the surface
   it means that levee collapse is already in progress.
   
   # Data-driven vs. finite element.
   Data-driven methods are model-free and include machine learning and
   statistical techniques, whereas finite-element analysis is a model-based
   method. One advantage of data-driven methods are that they do not require
   information about physical parameters of the monitored system. As opposed to
   finite-element analysis which in the case of levee condition monitoring
   requires parameters such as slope geometry and soil properties. The
   model-based methods provide more information about the monitored object, but
   are more expensive to evaluate and thus difficult to use for real-time
   condition assessment.
   
   # Combination of methods.
   In this paper the data-driven and finite-element components of the system
   which were developed are referred to as the Artificial Intelligence (AI) and
   Computer Model (CM) respectively. The AI and CM can be combined in two ways.
   In the first case the CM is used for data generation. Data is generated by
   the CM corresponding to normal and abnormal conditions. The normal behaviour
   data is used to train the AI and both the normal and abnormal behaviour data
   can be used for testing the AI. In the second case shown in TODO:FIG the CM
   is used for validation of the alarms generated by the AI. If the AI detects
   abnormal behaviour then the CM is run to confirm the result. If the AI was
   correct a warning is raised, else the new data point is used to retrain the
   AI.
   
   # Finite element analysis.
   The paper includes a section which demonstrates the applicability of FEM for
   prediction tasks. Real sensor values (collected from an experiment where a
   constructed levee was intentionaly collapsed) are compared to virtual sensor
   values generated by the CM. Figure TODO:REF it can be clearly seen how the
   real and virtual sensor values deviate prior to collapse.
** Flood early warning system: design, implementation and computational modules.
   # Decision support system.
   In TODO:REF a prototype of an flood early warning system (EWS) is presented
   as developed within the UrbanFlood FP7 project. This system monitors sensors
   installed in flood defenses, detects sensor signal abnormalities, calculates
   failure probability of the flood defense, and simulates failure scenarios.
   All of this information is made available online as part of a DSS to help the
   relevant figure of authority make an informed decision in case of emergency
   or routine assessment.
   
   # Relevant components of the EWS.
   Some requirements are listed which must be taken into account in the design
   of an EWS, these include:
   - sensor equipment design, installation and technical maintenance
   - sensor data transmission, filtering and analysis
   - computational models and simulation components
   - interactive visualization technologies
   - remote access to the system
   Thus it is clear that the development of an EWS or DSS consists of much more
   than the development of the software components, but must also take into
   account the installation of hardware and the transmission of information
   between components of the system.
  
   # Organization of the system.
   The EWS consists of a number of interacting components. The /Sensor
   Monitoring/ module receives data from the installed sensors which are then
   filtered by the /AI Anomaly Detector/. In case an abnormality is detected the
   /Reliability Analysis/ calculates the probability of failure. If the failure
   probability is high then the /Breach Simulator/ predicts the dynamics of the
   dike failure. The response which is calculated beginning with the /AI Anomaly
   Detector/ and ending with the /Breaching Simulator/ is a fast response i.e.
   the response is calculated quickly to be available to the decision maker
   without delay. The /Virtual Dike/ module is additionaly available for the
   purpose of simulation by expert users. The fast response and the response
   from the /Virtual Dike/ module are both fed to the /Flood Simulator/ which
   models the flooding dynamics, this information is sent to the decision
   support system to be made available to the decision maker.
** A clustering approach for structural health monitoring on bridges
   # Introduction. 
   In this paper TODO:REF a clustering based approach is presented to group
   substructures or joints with similar behaviour and to detect abnormal or
   damaged ones. The presented approach is based on the simple idea that a
   sensor located at a damaged substructure or joint will record responses that
   are significantly different from sensors at undamaged points on the bridge.

   # Collected data.
   The approach was applied to data collected from 2,400 tri-axial
   accelerometers installed on 800 jack arches on the Sydney Harbour Bridge. An
   /event/ is defined as a time period in which a vehicle is driving across a
   joint. A pre-set threshold is set to trigger the recording of the responses
   by each sensor, each event is then represented by a vector of samples $X$.

   # Normalisation.
   Prior to performing any abnormality detection the data is preprocessed. First
   each event data is transformed into a feature $V_i = |A_i| - |A_r|$ where
   $A_i$ is the instantaneous acceleration at the $i$th sample and $A_r$ is the
   "rest vector" or average of the first 100 samples. The event data is then
   normalised as $X = \frac{V - \mu(V)}{\sigma(V)}$.
   
   # Outlier removal.
   After normalisation of the event data, k-nearest neighbours is applied for
   outlier removal. One might consider that outliers are useful in the detection
   of abnormal conditions, since they represent abnormal responses. However if
   outlying data per joint are removed, then a greater level of confidence can
   be had when an abnormal condition is detected knowing that the result is not
   based on any outliers. In this outlier removal step the sum of the energy in
   time domain is calculated for event data as $E(X) = \sum_i |x_i|^2$. Then for
   every iteration of k-nearest neighbours, the $k$ closest neighbours to the
   mean of the enery of the joint's signals $\mu_{joint}$ is calculated.
   
   # Tranform and clustering metric.
   The event data is then transformed from the time domain into a series of
   frequencies using the Fast Fourier Transform (FFT), such that the original
   vibration data is now represented as sequence that determines the importance
   of each frequency component in the signal. After this transformation a
   distance metric is calculated for each pair of event signals, this metric is
   used for k-means clustering of the data for anomaly detection. The distance
   metric used is the Euclidean distance: $dist(X, Y) = ||X - Y|| = \sqrt{\sum
   (x_i - y_i)^2}$.
   
   # Event based clustering.
   Two clustering methods were applied, event-based and joint-based. In the
   event-based clustering experiment it was known beforehand that joint 4 was
   damaged. All event data was clustered using k-means clustering with $K = 2$
   which resulted in a big cluster containing 23,849 events and a smaller
   cluster of 4662 events mostly located in joint 4, detecting the damaged
   joint. In case there is no knowledge of abnormal behaviour then this method
   can be used to separate outliers and obtain a profile of normal behaviour.
   
   # Joint-based clustering.
   In joint-based clustering a pairwise map of distances is calculated between
   joint representatives. A joint representative is calculated as the mean of
   the values of all event data for a joint, after the outlier removal phase.
   Two experiments were conducted. One experiment consisted only of 6 joints,
   including the damaged joint 4. The clustering method detected the damaged
   joint as can be seen in TODO:FIG. The second experiment was run on data from
   71 joints. The resulting map can be seen in TODO:FIG which accuratley
   detected the damaged joint 135.
* Background material
  # Material without a specific paper e.g. FEM, bridges.
* Condition classification
  # How the classification model is built and how it performs.
** Components
   # Overview of components and how they interact.
** Simulated sensor measurements
   # How sensor measurements are simulated.
*** Neccessary data
    # What data is neccessary and how it was collected.
*** Finite element model
    # How the FEM is used to simulate sensor measurements.
** Condition classification
*** Test setup
    # How to test the model's classification ability.
*** Classification model
    # The model used for classification.
*** Sensor placement
    # The optimal placement for classification.
*** Uncertainty
    # What is the uncertainty in the system.
* Decision support system
  # The suggested system DSS.
** Outline
   # Outline of what is required to install the DSS.
** Cost-benefit analysis
   # What are costs and benefit of installing the DSS.
** Generalizing beyond bridge 705
   # What is required to move to another bridge.
* Conclusion
